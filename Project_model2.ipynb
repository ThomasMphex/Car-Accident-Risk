{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/clhq9k9140jfd9jjcjjjpqvh0000gn/T/ipykernel_64133/412493871.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  accidents = pd.read_csv(\"Accident_Information.csv\", encoding = 'Latin1')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>1st_Road_Class</th>\n",
       "      <th>1st_Road_Number</th>\n",
       "      <th>2nd_Road_Class</th>\n",
       "      <th>2nd_Road_Number</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Carriageway_Hazards</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n",
       "      <th>...</th>\n",
       "      <th>Police_Force</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>Road_Type</th>\n",
       "      <th>Special_Conditions_at_Site</th>\n",
       "      <th>Speed_limit</th>\n",
       "      <th>Time</th>\n",
       "      <th>Urban_or_Rural_Area</th>\n",
       "      <th>Weather_Conditions</th>\n",
       "      <th>Year</th>\n",
       "      <th>InScotland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200501BS00001</td>\n",
       "      <td>A</td>\n",
       "      <td>3218.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Serious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-04</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Metropolitan Police</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17:42</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Raining no high winds</td>\n",
       "      <td>2005</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200501BS00002</td>\n",
       "      <td>B</td>\n",
       "      <td>450.0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-05</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Metropolitan Police</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Dual carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17:36</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2005</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200501BS00003</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Metropolitan Police</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>00:15</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2005</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200501BS00004</td>\n",
       "      <td>A</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-07</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Metropolitan Police</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10:35</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2005</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200501BS00005</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-10</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Metropolitan Police</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21:13</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2005</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047251</th>\n",
       "      <td>2017984121017</td>\n",
       "      <td>A(M)</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dumfries and Galloway</td>\n",
       "      <td>Frost or ice</td>\n",
       "      <td>Dual carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Other</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047252</th>\n",
       "      <td>2017984121217</td>\n",
       "      <td>C</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Pedestrian in carriageway - not injured</td>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dumfries and Galloway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047253</th>\n",
       "      <td>2017984121717</td>\n",
       "      <td>A(M)</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dumfries and Galloway</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>Dual carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>13:30</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047254</th>\n",
       "      <td>2017984122317</td>\n",
       "      <td>A</td>\n",
       "      <td>708.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dumfries and Galloway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>18:00</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047255</th>\n",
       "      <td>2017984122617</td>\n",
       "      <td>B</td>\n",
       "      <td>721.0</td>\n",
       "      <td>B</td>\n",
       "      <td>724.0</td>\n",
       "      <td>Serious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Dumfries and Galloway</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Fog or mist</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2047256 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accident_Index 1st_Road_Class  1st_Road_Number 2nd_Road_Class  \\\n",
       "0        200501BS00001              A           3218.0            NaN   \n",
       "1        200501BS00002              B            450.0              C   \n",
       "2        200501BS00003              C              0.0            NaN   \n",
       "3        200501BS00004              A           3220.0            NaN   \n",
       "4        200501BS00005   Unclassified              0.0            NaN   \n",
       "...                ...            ...              ...            ...   \n",
       "2047251  2017984121017           A(M)             74.0            NaN   \n",
       "2047252  2017984121217              C             69.0            NaN   \n",
       "2047253  2017984121717           A(M)             74.0   Unclassified   \n",
       "2047254  2017984122317              A            708.0            NaN   \n",
       "2047255  2017984122617              B            721.0              B   \n",
       "\n",
       "         2nd_Road_Number Accident_Severity  \\\n",
       "0                    0.0           Serious   \n",
       "1                    0.0            Slight   \n",
       "2                    0.0            Slight   \n",
       "3                    0.0            Slight   \n",
       "4                    0.0            Slight   \n",
       "...                  ...               ...   \n",
       "2047251              0.0            Slight   \n",
       "2047252              0.0            Slight   \n",
       "2047253              0.0            Slight   \n",
       "2047254              0.0            Slight   \n",
       "2047255            724.0           Serious   \n",
       "\n",
       "                             Carriageway_Hazards        Date Day_of_Week  \\\n",
       "0                                            NaN  2005-01-04     Tuesday   \n",
       "1                                            NaN  2005-01-05   Wednesday   \n",
       "2                                            NaN  2005-01-06    Thursday   \n",
       "3                                            NaN  2005-01-07      Friday   \n",
       "4                                            NaN  2005-01-10      Monday   \n",
       "...                                          ...         ...         ...   \n",
       "2047251                                      NaN  2017-12-17      Sunday   \n",
       "2047252  Pedestrian in carriageway - not injured  2017-12-15      Friday   \n",
       "2047253                                      NaN  2017-12-18      Monday   \n",
       "2047254                                      NaN  2017-07-18     Tuesday   \n",
       "2047255                                      NaN  2017-12-20   Wednesday   \n",
       "\n",
       "         Did_Police_Officer_Attend_Scene_of_Accident  ...  \\\n",
       "0                                                1.0  ...   \n",
       "1                                                1.0  ...   \n",
       "2                                                1.0  ...   \n",
       "3                                                1.0  ...   \n",
       "4                                                1.0  ...   \n",
       "...                                              ...  ...   \n",
       "2047251                                          1.0  ...   \n",
       "2047252                                          2.0  ...   \n",
       "2047253                                          1.0  ...   \n",
       "2047254                                          1.0  ...   \n",
       "2047255                                          1.0  ...   \n",
       "\n",
       "                  Police_Force Road_Surface_Conditions           Road_Type  \\\n",
       "0          Metropolitan Police             Wet or damp  Single carriageway   \n",
       "1          Metropolitan Police                     Dry    Dual carriageway   \n",
       "2          Metropolitan Police                     Dry  Single carriageway   \n",
       "3          Metropolitan Police                     Dry  Single carriageway   \n",
       "4          Metropolitan Police             Wet or damp  Single carriageway   \n",
       "...                        ...                     ...                 ...   \n",
       "2047251  Dumfries and Galloway            Frost or ice    Dual carriageway   \n",
       "2047252  Dumfries and Galloway                     Dry  Single carriageway   \n",
       "2047253  Dumfries and Galloway             Wet or damp    Dual carriageway   \n",
       "2047254  Dumfries and Galloway                     Dry  Single carriageway   \n",
       "2047255  Dumfries and Galloway             Wet or damp  Single carriageway   \n",
       "\n",
       "        Special_Conditions_at_Site Speed_limit   Time  Urban_or_Rural_Area  \\\n",
       "0                              NaN        30.0  17:42                Urban   \n",
       "1                              NaN        30.0  17:36                Urban   \n",
       "2                              NaN        30.0  00:15                Urban   \n",
       "3                              NaN        30.0  10:35                Urban   \n",
       "4                              NaN        30.0  21:13                Urban   \n",
       "...                            ...         ...    ...                  ...   \n",
       "2047251                        NaN        70.0  11:30                Rural   \n",
       "2047252                        NaN        20.0  13:00                Urban   \n",
       "2047253                        NaN        70.0  13:30                Rural   \n",
       "2047254                        NaN        60.0  18:00                Rural   \n",
       "2047255                        NaN        40.0  13:00                Rural   \n",
       "\n",
       "            Weather_Conditions  Year InScotland  \n",
       "0        Raining no high winds  2005         No  \n",
       "1           Fine no high winds  2005         No  \n",
       "2           Fine no high winds  2005         No  \n",
       "3           Fine no high winds  2005         No  \n",
       "4           Fine no high winds  2005         No  \n",
       "...                        ...   ...        ...  \n",
       "2047251                  Other  2017        Yes  \n",
       "2047252     Fine no high winds  2017        Yes  \n",
       "2047253     Fine no high winds  2017        Yes  \n",
       "2047254     Fine no high winds  2017        Yes  \n",
       "2047255            Fog or mist  2017        Yes  \n",
       "\n",
       "[2047256 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents = pd.read_csv(\"Accident_Information.csv\", encoding = 'Latin1')\n",
    "vehicles = pd.read_csv(\"Vehicle_Information.csv\", encoding = 'Latin1')\n",
    "accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_merged = pd.merge(accidents,vehicles, on = \"Accident_Index\", how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>1st_Road_Class</th>\n",
       "      <th>1st_Road_Number</th>\n",
       "      <th>2nd_Road_Class</th>\n",
       "      <th>2nd_Road_Number</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Carriageway_Hazards</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n",
       "      <th>...</th>\n",
       "      <th>Skidding_and_Overturning</th>\n",
       "      <th>Towing_and_Articulation</th>\n",
       "      <th>Vehicle_Leaving_Carriageway</th>\n",
       "      <th>Vehicle_Location.Restricted_Lane</th>\n",
       "      <th>Vehicle_Manoeuvre</th>\n",
       "      <th>Vehicle_Reference</th>\n",
       "      <th>Vehicle_Type</th>\n",
       "      <th>Was_Vehicle_Left_Hand_Drive</th>\n",
       "      <th>X1st_Point_of_Impact</th>\n",
       "      <th>Year_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200501BS00002</td>\n",
       "      <td>B</td>\n",
       "      <td>450.0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-05</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slowing or stopping</td>\n",
       "      <td>1</td>\n",
       "      <td>Bus or coach (17 or more pass seats)</td>\n",
       "      <td>No</td>\n",
       "      <td>Nearside</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200501BS00003</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead right-hand bend</td>\n",
       "      <td>1</td>\n",
       "      <td>Bus or coach (17 or more pass seats)</td>\n",
       "      <td>No</td>\n",
       "      <td>Nearside</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200501BS00004</td>\n",
       "      <td>A</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-07</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Front</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200501BS00005</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-10</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Skidded</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Motorcycle 125cc and under</td>\n",
       "      <td>No</td>\n",
       "      <td>Front</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200501BS00006</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005-01-11</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Moving off</td>\n",
       "      <td>1</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Did not impact</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058403</th>\n",
       "      <td>2016984131116</td>\n",
       "      <td>B</td>\n",
       "      <td>7020.0</td>\n",
       "      <td>C</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Did not leave carriageway</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Front</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058404</th>\n",
       "      <td>2016984131116</td>\n",
       "      <td>B</td>\n",
       "      <td>7020.0</td>\n",
       "      <td>C</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Offside</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>2</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Front</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058405</th>\n",
       "      <td>2016984131216</td>\n",
       "      <td>A(M)</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-27</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Offside on to central reservation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Goods 7.5 tonnes mgw and over</td>\n",
       "      <td>No</td>\n",
       "      <td>Offside</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058406</th>\n",
       "      <td>2016984131316</td>\n",
       "      <td>B</td>\n",
       "      <td>724.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Skidded</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Offside</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Front</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058407</th>\n",
       "      <td>2016984133416</td>\n",
       "      <td>A(M)</td>\n",
       "      <td>74.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Slight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Skidded and overturned</td>\n",
       "      <td>No tow/articulation</td>\n",
       "      <td>Nearside</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Going ahead other</td>\n",
       "      <td>1</td>\n",
       "      <td>Car</td>\n",
       "      <td>No</td>\n",
       "      <td>Nearside</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2058408 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accident_Index 1st_Road_Class  1st_Road_Number 2nd_Road_Class  \\\n",
       "0        200501BS00002              B            450.0              C   \n",
       "1        200501BS00003              C              0.0            NaN   \n",
       "2        200501BS00004              A           3220.0            NaN   \n",
       "3        200501BS00005   Unclassified              0.0            NaN   \n",
       "4        200501BS00006   Unclassified              0.0            NaN   \n",
       "...                ...            ...              ...            ...   \n",
       "2058403  2016984131116              B           7020.0              C   \n",
       "2058404  2016984131116              B           7020.0              C   \n",
       "2058405  2016984131216           A(M)             74.0            NaN   \n",
       "2058406  2016984131316              B            724.0            NaN   \n",
       "2058407  2016984133416           A(M)             74.0            NaN   \n",
       "\n",
       "         2nd_Road_Number Accident_Severity Carriageway_Hazards        Date  \\\n",
       "0                    0.0            Slight                 NaN  2005-01-05   \n",
       "1                    0.0            Slight                 NaN  2005-01-06   \n",
       "2                    0.0            Slight                 NaN  2005-01-07   \n",
       "3                    0.0            Slight                 NaN  2005-01-10   \n",
       "4                    0.0            Slight                 NaN  2005-01-11   \n",
       "...                  ...               ...                 ...         ...   \n",
       "2058403             49.0            Slight                 NaN  2016-11-01   \n",
       "2058404             49.0            Slight                 NaN  2016-11-01   \n",
       "2058405              0.0            Slight                 NaN  2016-10-27   \n",
       "2058406              0.0            Slight                 NaN  2016-10-29   \n",
       "2058407              0.0            Slight                 NaN  2016-12-25   \n",
       "\n",
       "        Day_of_Week  Did_Police_Officer_Attend_Scene_of_Accident  ...  \\\n",
       "0         Wednesday                                          1.0  ...   \n",
       "1          Thursday                                          1.0  ...   \n",
       "2            Friday                                          1.0  ...   \n",
       "3            Monday                                          1.0  ...   \n",
       "4           Tuesday                                          1.0  ...   \n",
       "...             ...                                          ...  ...   \n",
       "2058403     Tuesday                                          1.0  ...   \n",
       "2058404     Tuesday                                          1.0  ...   \n",
       "2058405    Thursday                                          1.0  ...   \n",
       "2058406    Saturday                                          1.0  ...   \n",
       "2058407      Sunday                                          1.0  ...   \n",
       "\n",
       "        Skidding_and_Overturning Towing_and_Articulation  \\\n",
       "0                            NaN     No tow/articulation   \n",
       "1                            NaN     No tow/articulation   \n",
       "2                            NaN     No tow/articulation   \n",
       "3                        Skidded     No tow/articulation   \n",
       "4                            NaN     No tow/articulation   \n",
       "...                          ...                     ...   \n",
       "2058403                      NaN     No tow/articulation   \n",
       "2058404                      NaN     No tow/articulation   \n",
       "2058405                      NaN     No tow/articulation   \n",
       "2058406                  Skidded     No tow/articulation   \n",
       "2058407   Skidded and overturned     No tow/articulation   \n",
       "\n",
       "               Vehicle_Leaving_Carriageway Vehicle_Location.Restricted_Lane  \\\n",
       "0                Did not leave carriageway                              0.0   \n",
       "1                Did not leave carriageway                              0.0   \n",
       "2                Did not leave carriageway                              0.0   \n",
       "3                Did not leave carriageway                              0.0   \n",
       "4                Did not leave carriageway                              0.0   \n",
       "...                                    ...                              ...   \n",
       "2058403          Did not leave carriageway                              0.0   \n",
       "2058404                            Offside                              0.0   \n",
       "2058405  Offside on to central reservation                              0.0   \n",
       "2058406                            Offside                              0.0   \n",
       "2058407                           Nearside                              0.0   \n",
       "\n",
       "                   Vehicle_Manoeuvre Vehicle_Reference  \\\n",
       "0                Slowing or stopping                 1   \n",
       "1        Going ahead right-hand bend                 1   \n",
       "2                  Going ahead other                 1   \n",
       "3                  Going ahead other                 1   \n",
       "4                         Moving off                 1   \n",
       "...                              ...               ...   \n",
       "2058403            Going ahead other                 1   \n",
       "2058404            Going ahead other                 2   \n",
       "2058405            Going ahead other                 1   \n",
       "2058406            Going ahead other                 1   \n",
       "2058407            Going ahead other                 1   \n",
       "\n",
       "                                 Vehicle_Type  Was_Vehicle_Left_Hand_Drive  \\\n",
       "0        Bus or coach (17 or more pass seats)                           No   \n",
       "1        Bus or coach (17 or more pass seats)                           No   \n",
       "2                                         Car                           No   \n",
       "3                  Motorcycle 125cc and under                           No   \n",
       "4                                         Car                           No   \n",
       "...                                       ...                          ...   \n",
       "2058403                                   Car                           No   \n",
       "2058404                                   Car                           No   \n",
       "2058405         Goods 7.5 tonnes mgw and over                           No   \n",
       "2058406                                   Car                           No   \n",
       "2058407                                   Car                           No   \n",
       "\n",
       "         X1st_Point_of_Impact Year_y  \n",
       "0                    Nearside   2005  \n",
       "1                    Nearside   2005  \n",
       "2                       Front   2005  \n",
       "3                       Front   2005  \n",
       "4              Did not impact   2005  \n",
       "...                       ...    ...  \n",
       "2058403                 Front   2016  \n",
       "2058404                 Front   2016  \n",
       "2058405               Offside   2016  \n",
       "2058406                 Front   2016  \n",
       "2058407              Nearside   2016  \n",
       "\n",
       "[2058408 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/clhq9k9140jfd9jjcjjjpqvh0000gn/T/ipykernel_64133/231236371.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_raw = pd.read_csv(\"train_raw.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv(\"train_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after label encoding:\n",
      "  Accident_Index  1st_Road_Class_x  1st_Road_Number  2nd_Road_Class_x  \\\n",
      "0  200501BS00002                 2            450.0                 3   \n",
      "1  200501BS00003                 3              0.0                 6   \n",
      "2  200501BS00004                 0           3220.0                 6   \n",
      "3  200501BS00005                 5              0.0                 6   \n",
      "4  200501BS00006                 5              0.0                 6   \n",
      "5  200501BS00006                 5              0.0                 6   \n",
      "6  200501BS00007                 3              0.0                 5   \n",
      "7  200501BS00007                 3              0.0                 5   \n",
      "8  200501BS00009                 0            315.0                 6   \n",
      "9  200501BS00012                 0              4.0                 2   \n",
      "\n",
      "   2nd_Road_Number  Accident_Severity_x        Date  Day_of_Week_x  \\\n",
      "0              0.0                    2  1104883200              6   \n",
      "1              0.0                    2  1104969600              4   \n",
      "2              0.0                    2  1105056000              0   \n",
      "3              0.0                    2  1105315200              1   \n",
      "4              0.0                    2  1105401600              5   \n",
      "5              0.0                    2  1105401600              5   \n",
      "6              0.0                    2  1105574400              4   \n",
      "7              0.0                    2  1105574400              4   \n",
      "8              0.0                    2  1105660800              0   \n",
      "9            325.0                    2  1105833600              3   \n",
      "\n",
      "   Did_Police_Officer_Attend_Scene_of_Accident_x  Junction_Control_x  ...  \\\n",
      "0                                              0                   1  ...   \n",
      "1                                              0                   2  ...   \n",
      "2                                              0                   2  ...   \n",
      "3                                              0                   2  ...   \n",
      "4                                              0                   2  ...   \n",
      "5                                              0                   2  ...   \n",
      "6                                              0                   3  ...   \n",
      "7                                              0                   3  ...   \n",
      "8                                              0                   2  ...   \n",
      "9                                              0                   1  ...   \n",
      "\n",
      "   Junction_Location  Sex_of_Driver  Towing_and_Articulation  \\\n",
      "0                  7              2                        4   \n",
      "1                  9              2                        4   \n",
      "2                  9              1                        4   \n",
      "3                  9              2                        4   \n",
      "4                  9              2                        4   \n",
      "5                  9              1                        4   \n",
      "6                  0              2                        4   \n",
      "7                  0              2                        4   \n",
      "8                  9              2                        4   \n",
      "9                  8              2                        4   \n",
      "\n",
      "   Vehicle_Leaving_Carriageway  Vehicle_Manoeuvre  Vehicle_Reference  \\\n",
      "0                            1                 12                  1   \n",
      "1                            1                  5                  1   \n",
      "2                            1                  4                  1   \n",
      "3                            1                  4                  1   \n",
      "4                            1                  6                  1   \n",
      "5                            1                  4                  2   \n",
      "6                            1                  4                  1   \n",
      "7                            1                 10                  2   \n",
      "8                            2                  4                  1   \n",
      "9                            1                  4                  1   \n",
      "\n",
      "   Vehicle_Type  Was_Vehicle_Left_Hand_Drive  X1st_Point_of_Impact  Year_y  \n",
      "0             1                            1                     4    2005  \n",
      "1             1                            1                     4    2005  \n",
      "2             2                            1                     3    2005  \n",
      "3            11                            1                     3    2005  \n",
      "4             2                            1                     2    2005  \n",
      "5            11                            1                     2    2005  \n",
      "6            11                            1                     3    2005  \n",
      "7             2                            1                     0    2005  \n",
      "8             2                            1                     3    2005  \n",
      "9             2                            1                     3    2005  \n",
      "\n",
      "[10 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of categorical columns\n",
    "categorical_columns = ['Age_Band_of_Driver', 'Driver_Home_Area_Type', 'Journey_Purpose_of_Driver', \n",
    "                       'Junction_Location','Sex_of_Driver','Towing_and_Articulation',\n",
    "                       'Vehicle_Leaving_Carriageway','Vehicle_Manoeuvre', \n",
    "                       'Vehicle_Type', 'Was_Vehicle_Left_Hand_Drive', 'X1st_Point_of_Impact']\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    train_raw[col] = le.fit_transform(train_raw[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"DataFrame after label encoding:\")\n",
    "print(train_raw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Sample data (replace this with your actual dataset)\n",
    "# # train_raw = pd.read_csv('path/to/train_raw.csv')\n",
    "\n",
    "# # Define features and target\n",
    "# features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'LSOA_of_Accident_Location_x', 'Year_x']\n",
    "# target = 'Accident_Severity_x'\n",
    "\n",
    "# # Split the data\n",
    "# X = train_raw[features]\n",
    "# y = train_raw[target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocessing for numerical data\n",
    "# numeric_features = ['1st_Road_Number', '2nd_Road_Number', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Year_x']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# # Preprocessing for categorical data\n",
    "# categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'LSOA_of_Accident_Location_x']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# # Bundle preprocessing for numerical and categorical data\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# # Define the model\n",
    "# model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                         ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# # Define parameter grid for Grid Search\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [50, 100, 200],\n",
    "#     'classifier__max_depth': [None, 10, 20, 30],\n",
    "#     'classifier__min_samples_split': [2, 5, 10],\n",
    "#     'classifier__min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# # Grid Search\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best model\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)\n",
    "# print(\"Accuracy of the model: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# # Define features and target\n",
    "# features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Longitude', 'Latitude', 'LSOA_of_Accident_Location_x']\n",
    "# target = 'Accident_Severity_x'\n",
    "\n",
    "# # Split the data\n",
    "# X = train_raw[features]\n",
    "# y = train_raw[target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocessing for numerical data\n",
    "# numeric_features = ['1st_Road_Number', '2nd_Road_Number', 'Longitude', 'Latitude']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# # Preprocessing for categorical data\n",
    "# categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'LSOA_of_Accident_Location_x']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# # Bundle preprocessing for numerical and categorical data\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# # Define the model\n",
    "# model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# # Create a pipeline that preprocesses the data, applies SMOTE, and then the model\n",
    "# pipeline = ImbPipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('smote', SMOTE(random_state=42)),\n",
    "#     ('classifier', model)\n",
    "# ])\n",
    "\n",
    "# # Define parameter grid for Grid Search\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [100, 200],\n",
    "#     'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'classifier__max_depth': [3, 5, 7],\n",
    "#     'classifier__subsample': [0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# # Initialize Grid Search\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_macro')\n",
    "\n",
    "# # Train the model\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(f'Test Accuracy: {grid_search.score(X_test, y_test) * 100:.2f}%')\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # Print the best parameters\n",
    "# print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 44.45%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.62      0.04      5456\n",
      "           1       0.16      0.23      0.19     53206\n",
      "           2       0.89      0.47      0.62    353020\n",
      "\n",
      "    accuracy                           0.44    411682\n",
      "   macro avg       0.36      0.44      0.28    411682\n",
      "weighted avg       0.79      0.44      0.56    411682\n",
      "\n",
      "[[  3401    800   1255]\n",
      " [ 21956  12291  18959]\n",
      " [121465  64254 167301]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Define features and target\n",
    "features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Longitude', 'Latitude', 'LSOA_of_Accident_Location_x']\n",
    "target = 'Accident_Severity_x'\n",
    "\n",
    "# Split the data\n",
    "X = train_raw[features]\n",
    "y = train_raw[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights for the target variable\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_features = ['1st_Road_Number', '2nd_Road_Number', 'Longitude', 'Latitude']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'LSOA_of_Accident_Location_x']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Define the model with class weights\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, subsample=0.8)\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then applies the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=class_weights[y_train])\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Test Accuracy: {pipeline.score(X_test, y_test) * 100:.2f}%')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.01      0.01      5456\n",
      "           1       1.00      0.00      0.00     53206\n",
      "           2       0.86      1.00      0.92    353020\n",
      "\n",
      "    accuracy                           0.86    411682\n",
      "   macro avg       0.78      0.34      0.31    411682\n",
      "weighted avg       0.87      0.86      0.79    411682\n",
      "\n",
      "[[    28      0   5428]\n",
      " [     5     12  53189]\n",
      " [    25      0 352995]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define features and target\n",
    "features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Longitude', 'Latitude', 'LSOA_of_Accident_Location_x']\n",
    "target = 'Accident_Severity_x'\n",
    "\n",
    "# Split the data\n",
    "X = train_raw[features]\n",
    "y = train_raw[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_features = ['1st_Road_Number', '2nd_Road_Number', 'Longitude', 'Latitude']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'LSOA_of_Accident_Location_x']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, subsample=0.8)\n",
    "\n",
    "# Create a pipeline that preprocesses the data and then applies the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Test Accuracy: {pipeline.score(X_test, y_test) * 100:.2f}%')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'accident_severity_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save the model to a file\n",
    "with open('accident_severity_model.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "print(\"Model saved as 'accident_severity_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Accident Severity: 2\n"
     ]
    }
   ],
   "source": [
    "def predict_accident_severity(pipeline):\n",
    "    \"\"\"\n",
    "    Function to predict accident severity based on user input.\n",
    "    \"\"\"\n",
    "    # User input for each feature\n",
    "    user_input = {\n",
    "        '1st_Road_Number': float(input(\"Enter 1st Road Number: \")),\n",
    "        '2nd_Road_Number': float(input(\"Enter 2nd Road Number: \")),\n",
    "        'Date': input(\"Enter Date (YYYY-MM-DD): \"),\n",
    "        'Local_Authority_(District)_x': input(\"Enter Local Authority (District): \"),\n",
    "        'Local_Authority_(Highway)_x': input(\"Enter Local Authority (Highway): \"),\n",
    "        'Latitude': float(input(\"Enter Latitude: \")),\n",
    "        'Longitude': float(input(\"Enter Longitude: \")),\n",
    "        'LSOA_of_Accident_Location_x': input(\"Enter LSOA of Accident Location: \"),\n",
    "    }\n",
    "    \n",
    "    # Convert the user input into a DataFrame\n",
    "    user_df = pd.DataFrame([user_input])\n",
    "    \n",
    "    # Convert the Date to numeric as done previously\n",
    "    user_df['Date'] = pd.to_datetime(user_df['Date'])\n",
    "    user_df['Date'] = (user_df['Date'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "    \n",
    "    # Predict the accident severity\n",
    "    prediction = pipeline.predict(user_df)\n",
    "    \n",
    "    print(f\"Predicted Accident Severity: {prediction[0]}\")\n",
    "\n",
    "# Test the user input prediction function\n",
    "predict_accident_severity(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m105948/105948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1763s\u001b[0m 17ms/step - accuracy: 0.7615 - loss: 0.5429 - val_accuracy: 0.3230 - val_loss: 0.9669\n",
      "Epoch 2/5\n",
      "\u001b[1m105948/105948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1795s\u001b[0m 17ms/step - accuracy: 0.8484 - loss: 0.3392 - val_accuracy: 0.4661 - val_loss: 0.8804\n",
      "Epoch 3/5\n",
      "\u001b[1m105948/105948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1701s\u001b[0m 16ms/step - accuracy: 0.8688 - loss: 0.3088 - val_accuracy: 0.5750 - val_loss: 0.7154\n",
      "Epoch 4/5\n",
      "\u001b[1m105948/105948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1692s\u001b[0m 16ms/step - accuracy: 0.8802 - loss: 0.2900 - val_accuracy: 0.6250 - val_loss: 0.7056\n",
      "Epoch 5/5\n",
      "\u001b[1m105948/105948\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2997s\u001b[0m 28ms/step - accuracy: 0.8885 - loss: 0.2751 - val_accuracy: 0.6604 - val_loss: 0.6446\n",
      "\u001b[1m12866/12866\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 471us/step\n",
      "Test Accuracy: 81.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50      5456\n",
      "           1       0.31      0.25      0.28     53206\n",
      "           2       0.89      0.91      0.90    353020\n",
      "\n",
      "    accuracy                           0.82    411682\n",
      "   macro avg       0.56      0.55      0.56    411682\n",
      "weighted avg       0.81      0.82      0.81    411682\n",
      "\n",
      "[[  2714    418   2324]\n",
      " [   520  13267  39419]\n",
      " [  2210  29304 321506]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define features and target\n",
    "features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Longitude', 'Latitude', 'LSOA_of_Accident_Location_x']\n",
    "target = 'Accident_Severity_x'\n",
    "\n",
    "# Split the data\n",
    "X = train_raw[features]\n",
    "y = train_raw[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_features = ['1st_Road_Number', '2nd_Road_Number', 'Longitude', 'Latitude']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'LSOA_of_Accident_Location_x']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_resampled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_resampled, y_train_resampled, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Test Accuracy: {model.evaluate(X_test, y_test, verbose=0)[1] * 100:.2f}%')\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Neural Network\n",
    "nn = MLPClassifier(hidden_layer_sizes=(64, 64), max_iter=20, random_state=42)\n",
    "\n",
    "# Gradient Boosting Machine\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, subsample=0.8)\n",
    "\n",
    "# Train Neural Network\n",
    "nn.fit(X_train, y_train)\n",
    "nn_preds_train = nn.predict_proba(X_train)\n",
    "nn_preds_test = nn.predict_proba(X_test)\n",
    "\n",
    "# Train GBM\n",
    "gbm.fit(X_train, y_train)\n",
    "gbm_preds_train = gbm.predict_proba(X_train)\n",
    "gbm_preds_test = gbm.predict_proba(X_test)\n",
    "\n",
    "# Stacking\n",
    "X_train_stack = np.hstack((nn_preds_train, gbm_preds_train))\n",
    "X_test_stack = np.hstack((nn_preds_test, gbm_preds_test))\n",
    "\n",
    "# Meta-model\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(X_train_stack, y_train)\n",
    "y_pred_stack = meta_model.predict(X_test_stack)\n",
    "\n",
    "# Evaluate\n",
    "print(f'Test Accuracy: {meta_model.score(X_test_stack, y_test) * 100:.2f}%')\n",
    "print(classification_report(y_test, y_pred_stack))\n",
    "print(confusion_matrix(y_test, y_pred_stack))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k= 'all')\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Define features and target\n",
    "# features = ['1st_Road_Number', '2nd_Road_Number', 'Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x', 'Latitude', 'Longitude', 'Year_x']\n",
    "# target = 'Accident_Severity_x'\n",
    "\n",
    "# # Split the data\n",
    "# X = train_raw[features]\n",
    "# y = train_raw[target]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocessing for numerical data\n",
    "# numeric_features = ['1st_Road_Number', '2nd_Road_Number','Latitude','Longitude', 'Year_x']\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())])\n",
    "\n",
    "# # Preprocessing for categorical data\n",
    "# categorical_features = ['Date', 'Local_Authority_(District)_x', 'Local_Authority_(Highway)_x']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# # Bundle preprocessing for numerical and categorical data\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train = preprocessor.fit_transform(X_train)\n",
    "# X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# # Convert target to categorical (one-hot encoding)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# # Build the ANN model\n",
    "# def build_ann(input_shape):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, input_shape=input_shape, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "#     optimizer = Adam(learning_rate=0.01) \n",
    "#     model.compile(optimizer=optimizer, \n",
    "#                   loss='categorical_crossentropy', \n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# input_shape = (X_train.shape[1],)\n",
    "# model = build_ann(input_shape)\n",
    "\n",
    "# # Calculate class weights\n",
    "# class_weights = {0: len(y) / (3 * np.bincount(y)[0]),\n",
    "#                  1: len(y) / (3 * np.bincount(y)[1]),\n",
    "#                  2: len(y) / (3 * np.bincount(y)[2])}\n",
    "\n",
    "# # Set early stopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model with class weights\n",
    "# history = model.fit(X_train, y_train, \n",
    "#                     validation_split=0.2, \n",
    "#                     epochs=50, \n",
    "#                     batch_size=32, \n",
    "#                     class_weight=class_weights,\n",
    "#                     callbacks=[early_stopping])\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# # Predict and evaluate further if needed\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # Additional evaluation metrics (optional)\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_test_classes, y_pred_classes))\n",
    "# print(confusion_matrix(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization and splitting completed.\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data normalization and splitting completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN model built and compiled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer and first hidden layer\n",
    "model.add(Dense(units= 32, activation='sigmoid', input_dim=X_train.shape[1]))\n",
    "\n",
    "# Add second hidden layer\n",
    "model.add(Dense(units= 16, activation='sigmoid'))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ANN model built and compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 532us/step - accuracy: 0.1283 - loss: -114.7594 - val_accuracy: 0.1296 - val_loss: -521.8502\n",
      "Epoch 2/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 568us/step - accuracy: 0.1284 - loss: -669.7657 - val_accuracy: 0.1296 - val_loss: -1110.4153\n",
      "Epoch 3/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 557us/step - accuracy: 0.1280 - loss: -1260.0397 - val_accuracy: 0.1296 - val_loss: -1699.0474\n",
      "Epoch 4/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 572us/step - accuracy: 0.1289 - loss: -1847.7656 - val_accuracy: 0.1296 - val_loss: -2287.5457\n",
      "Epoch 5/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 673us/step - accuracy: 0.1288 - loss: -2437.3679 - val_accuracy: 0.1296 - val_loss: -2876.0537\n",
      "Epoch 6/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 547us/step - accuracy: 0.1282 - loss: -3027.3247 - val_accuracy: 0.1296 - val_loss: -3464.5215\n",
      "Epoch 7/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 684us/step - accuracy: 0.1285 - loss: -3617.0740 - val_accuracy: 0.1296 - val_loss: -4052.9890\n",
      "Epoch 8/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 600us/step - accuracy: 0.1289 - loss: -4203.9170 - val_accuracy: 0.1296 - val_loss: -4641.5459\n",
      "Epoch 9/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 614us/step - accuracy: 0.1289 - loss: -4792.5977 - val_accuracy: 0.1296 - val_loss: -5230.2944\n",
      "Epoch 10/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 643us/step - accuracy: 0.1290 - loss: -5382.0972 - val_accuracy: 0.1296 - val_loss: -5818.8696\n",
      "Epoch 11/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 627us/step - accuracy: 0.1285 - loss: -5975.4912 - val_accuracy: 0.1296 - val_loss: -6407.3667\n",
      "Epoch 12/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 623us/step - accuracy: 0.1284 - loss: -6563.4868 - val_accuracy: 0.1296 - val_loss: -6995.9697\n",
      "Epoch 13/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 691us/step - accuracy: 0.1283 - loss: -7152.3433 - val_accuracy: 0.1296 - val_loss: -7583.9351\n",
      "Epoch 14/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 639us/step - accuracy: 0.1289 - loss: -7739.4028 - val_accuracy: 0.1296 - val_loss: -8171.0303\n",
      "Epoch 15/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 621us/step - accuracy: 0.1288 - loss: -8326.8506 - val_accuracy: 0.1296 - val_loss: -8757.9941\n",
      "Epoch 16/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 644us/step - accuracy: 0.1287 - loss: -8916.2988 - val_accuracy: 0.1296 - val_loss: -9345.3193\n",
      "Epoch 17/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 635us/step - accuracy: 0.1284 - loss: -9502.9834 - val_accuracy: 0.1296 - val_loss: -9931.6299\n",
      "Epoch 18/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 615us/step - accuracy: 0.1290 - loss: -10088.1895 - val_accuracy: 0.1296 - val_loss: -10519.2031\n",
      "Epoch 19/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 623us/step - accuracy: 0.1287 - loss: -10679.0947 - val_accuracy: 0.1296 - val_loss: -11106.4434\n",
      "Epoch 20/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 654us/step - accuracy: 0.1288 - loss: -11263.2471 - val_accuracy: 0.1296 - val_loss: -11693.9219\n",
      "Epoch 21/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 631us/step - accuracy: 0.1279 - loss: -11859.7158 - val_accuracy: 0.1296 - val_loss: -12280.8164\n",
      "Epoch 22/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 648us/step - accuracy: 0.1291 - loss: -12437.6240 - val_accuracy: 0.1296 - val_loss: -12867.6748\n",
      "Epoch 23/50\n",
      "\u001b[1m41169/41169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 628us/step - accuracy: 0.1288 - loss: -13027.1836 - val_accuracy: 0.1296 - val_loss: -13454.9873\n",
      "Epoch 24/50\n",
      "\u001b[1m23928/41169\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 651us/step - accuracy: 0.1288 - loss: -13542.4434"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m trace_function(\n\u001b[1;32m    133\u001b[0m     args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, tracing_options\u001b[38;5;241m=\u001b[39mtracing_options\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:230\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    220\u001b[0m current_func_context \u001b[38;5;241m=\u001b[39m function_context\u001b[38;5;241m.\u001b[39mmake_function_context(\n\u001b[1;32m    221\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mscope_type\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    224\u001b[0m capture_types \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    225\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mcapture_types\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_captures\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    228\u001b[0m )\n\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 230\u001b[0m     function_type_utils\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    231\u001b[0m         args,\n\u001b[1;32m    232\u001b[0m         kwargs,\n\u001b[1;32m    233\u001b[0m         capture_types,\n\u001b[1;32m    234\u001b[0m         tracing_options\u001b[38;5;241m.\u001b[39mpolymorphic_type,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39mlookup(\n\u001b[1;32m    240\u001b[0m       lookup_func_type, current_func_context\n\u001b[1;32m    241\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:375\u001b[0m, in \u001b[0;36mmake_canonicalized_monomorphic_type\u001b[0;34m(args, kwargs, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates function type given the function arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    370\u001b[0m     function_type_lib\u001b[38;5;241m.\u001b[39msanitize_arg_name(name): value\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    372\u001b[0m }\n\u001b[1;32m    374\u001b[0m function_type, type_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 375\u001b[0m     function_type_lib\u001b[38;5;241m.\u001b[39mcanonicalize_to_monomorphic(\n\u001b[1;32m    376\u001b[0m         args, kwargs, {}, capture_types, polymorphic_type\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_type, type_context\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:583\u001b[0m, in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    577\u001b[0m       parameters\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    578\u001b[0m           _make_validated_mono_param(kwarg_name, arg[kwarg_name],\n\u001b[1;32m    579\u001b[0m                                      Parameter\u001b[38;5;241m.\u001b[39mKEYWORD_ONLY, type_context,\n\u001b[1;32m    580\u001b[0m                                      poly_parameter\u001b[38;5;241m.\u001b[39mtype_constraint))\n\u001b[1;32m    581\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 583\u001b[0m         _make_validated_mono_param(name, arg, poly_parameter\u001b[38;5;241m.\u001b[39mkind,\n\u001b[1;32m    584\u001b[0m                                    type_context,\n\u001b[1;32m    585\u001b[0m                                    poly_parameter\u001b[38;5;241m.\u001b[39mtype_constraint))\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FunctionType(parameters, capture_types), type_context\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:522\u001b[0m, in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_validated_mono_param\u001b[39m(\n\u001b[1;32m    519\u001b[0m     name, value, kind, type_context, poly_type\n\u001b[1;32m    520\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Parameter:\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m   mono_type \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mfrom_value(value, type_context)\n\u001b[1;32m    524\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m poly_type \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mono_type\u001b[38;5;241m.\u001b[39mis_subtype_of(poly_type):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was expected to be of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoly_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmono_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:144\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mis_legacy_signature \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mTraceType):\n\u001b[1;32m    143\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mSupportsTracingProtocol):\n\u001b[1;32m    145\u001b[0m   generated_type \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generated_type, trace\u001b[38;5;241m.\u001b[39mTraceType):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/typing_extensions.py:589\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstance and class checks can only be used with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@runtime_checkable protocols\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m abc\u001b[38;5;241m.\u001b[39mABCMeta\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, other)\n\u001b[0;32m--> 589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Protocol:\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
